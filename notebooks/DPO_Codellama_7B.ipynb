{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1993db8e",
   "metadata": {},
   "source": [
    "# DPO Fine-Tuning — CodeLlama-7B\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| **Base model** | `codellama/CodeLlama-7b-hf` |\n",
    "| **Adaptation** | LoRA (r = 64, α = 64, bf16) |\n",
    "| **GPU** | H100 80 GB |\n",
    "| **Pipeline** | SFT → DPO |\n",
    "| **Dataset** | [`t4gandhi/llm_corrected_humaneval_dataset`](https://huggingface.co/datasets/t4gandhi/llm_corrected_humaneval_dataset) |\n",
    "| **Evaluation** | Pairwise preference accuracy on held-out test set (5-pass) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps {xformers} \"trl>=0.12,<0.14\" peft accelerate triton bitsandbytes\n",
    "!pip install --no-deps \"transformers>=4.50.3\"\n",
    "# restart runtime, then run from the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, warnings, torch\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"300\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── Precision configuration ──────────────────────────────────────────\n",
    "# TF32 on Ampere+ (compute capability >= 8.0): ~2x faster, same convergence.\n",
    "# Falls back to bf16 if TF32 unavailable, then fp16 as last resort.\n",
    "if torch.cuda.is_available():\n",
    "    _cc = torch.cuda.get_device_capability()\n",
    "    USE_TF32 = _cc[0] >= 8                         # Ampere / Hopper / Ada\n",
    "    USE_BF16 = torch.cuda.is_bf16_supported()\n",
    "    USE_FP16 = not USE_BF16\n",
    "    if USE_TF32:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32  = True\n",
    "    torch.backends.cudnn.benchmark = True           # auto-tune conv kernels\n",
    "    _vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU : {torch.cuda.get_device_name(0)} | VRAM {_vram:.0f} GB | \"\n",
    "          f\"CC {_cc[0]}.{_cc[1]} | TF32 {'ON' if USE_TF32 else 'off'} | \"\n",
    "          f\"{'bf16' if USE_BF16 else 'fp16'}\")\n",
    "else:\n",
    "    USE_TF32 = False; USE_BF16 = False; USE_FP16 = True\n",
    "\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "login(token=userdata.get(\"HF_TOKEN\"))\n",
    "\n",
    "import transformers.models.auto.modeling_auto as _auto_mod  # trl compat\n",
    "if not hasattr(_auto_mod, \"MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES\"):\n",
    "    _auto_mod.MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = {}\n",
    "\n",
    "MODEL_NAME     = \"codellama/CodeLlama-7b-hf\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT   = False\n",
    "SAMPLE_SIZE    = None\n",
    "RUN_SFT        = True\n",
    "RUN_DPO        = True\n",
    "SEED           = 3407\n",
    "NUM_PROC       = 8\n",
    "DL_WORKERS     = 4\n",
    "\n",
    "BATCH_SIZE     = 8\n",
    "GRAD_ACCUM     = 2    # eff. 16\n",
    "SFT_EPOCHS     = 1\n",
    "DPO_EPOCHS     = 15\n",
    "DPO_LR         = 2e-5\n",
    "PATIENCE       = 5\n",
    "EVAL_PASSES    = 5\n",
    "\n",
    "DPO_BATCH_SIZE = 4\n",
    "DPO_GRAD_ACCUM = 4    # eff. 16\n",
    "DPO_MAX_LEN    = 2048\n",
    "DPO_PROMPT_LEN = 1024\n",
    "\n",
    "# GPU-adaptive: override batch sizes based on available VRAM\n",
    "if torch.cuda.is_available():\n",
    "    _vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} | VRAM: {_vram:.0f} GB\")\n",
    "    if _vram >= 70:   BATCH_SIZE, GRAD_ACCUM, DPO_BATCH_SIZE, DPO_GRAD_ACCUM = 8, 2, 4, 4\n",
    "    elif _vram >= 35: BATCH_SIZE, GRAD_ACCUM, DPO_BATCH_SIZE, DPO_GRAD_ACCUM = 4, 4, 2, 8\n",
    "    elif _vram >= 20: BATCH_SIZE, GRAD_ACCUM, DPO_BATCH_SIZE, DPO_GRAD_ACCUM = 2, 8, 1, 16\n",
    "    else:             BATCH_SIZE, GRAD_ACCUM, DPO_BATCH_SIZE, DPO_GRAD_ACCUM = 2, 8, 1, 16\n",
    "    print(f\"SFT batch: {BATCH_SIZE}×{GRAD_ACCUM} (eff {BATCH_SIZE*GRAD_ACCUM}) | \"\n",
    "          f\"DPO batch: {DPO_BATCH_SIZE}×{DPO_GRAD_ACCUM} (eff {DPO_BATCH_SIZE*DPO_GRAD_ACCUM})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, time, json, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd, torch, matplotlib.pyplot as plt, seaborn as sns\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "FIGURE_DIR = Path(\"figures\"); FIGURE_DIR.mkdir(exist_ok=True)\n",
    "plt.rcParams.update({\"figure.dpi\": 150, \"savefig.dpi\": 300,\n",
    "                      \"savefig.bbox\": \"tight\", \"font.size\": 11})\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1,\n",
    "              rc={\"axes.spines.top\": False, \"axes.spines.right\": False})\n",
    "\n",
    "def gpu_mem():\n",
    "    if not torch.cuda.is_available(): return \"no GPU\"\n",
    "    return f\"{torch.cuda.memory_allocated()/1e9:.1f} GB (peak {torch.cuda.max_memory_allocated()/1e9:.1f} GB)\"\n",
    "\n",
    "def clear_gpu():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "def apply_chat_template(example, tokenizer=None, task=\"sft\"):\n",
    "    if task in (\"sft\", \"generation\"):\n",
    "        msgs = example[\"messages\"]\n",
    "        example[\"text\"] = (f\"<s>[INST] <<SYS>>\\n\\n<</SYS>>\\n\\n\"\n",
    "                           f\"{msgs[0]['content']}\\n[/INST] {msgs[1]['content']}</s>\")\n",
    "    elif task == \"dpo\":\n",
    "        user = example[\"chosen\"][0][\"content\"]\n",
    "        example[\"text_prompt\"]   = f\"<s>[INST] <<SYS>>\\n\\n<</SYS>>\\n\\n{user}\\n[/INST]\"\n",
    "        example[\"text_chosen\"]   = f\" {example['chosen'][1]['content']}</s>\"\n",
    "        example[\"text_rejected\"] = f\" {example['rejected'][1]['content']}</s>\"\n",
    "    return example\n",
    "\n",
    "INSTRUCTION = (\n",
    "    \"<instruction>\\n  <bullets>\\n\"\n",
    "    \"    <bullet>The following buggy code is a wrong implementation that contains one or more bugs.</bullet>\\n\"\n",
    "    \"    <bullet>Firstly, find all of the bugs within the buggy code. Make sure to quotate each part of the buggy code that contains a bug.</bullet>\\n\"\n",
    "    \"    <bullet>Afterwards, for each of the bugs, describe the issue with each part of the buggy code with the bug, and outline how to fix the issue.</bullet>\\n\"\n",
    "    \"    <bullet>Make sure your answer covers (1) all of the existing bugs, (2) do not hallucinate non-existing bugs, and (3) be concise as possible.</bullet>\\n\"\n",
    "    \"    <bullet>IMPORTANT!: While abiding by the above instructions, keep your answer as brief as possible.</bullet>\\n\"\n",
    "    \"  </bullets>\\n</instruction>\"\n",
    ")\n",
    "\n",
    "HF_DATASET = \"t4gandhi/llm_corrected_humaneval_dataset\"\n",
    "\n",
    "def _parse_score(val):\n",
    "    if val is None or (isinstance(val, float) and pd.isna(val)): return 0\n",
    "    s = str(val).strip()\n",
    "    return int(s[0]) if s and s != \"nan\" else 0\n",
    "\n",
    "def make_datasets(sample_size=None):\n",
    "    \"\"\"Build SFT and DPO splits from the HF preference dataset.\"\"\"\n",
    "    print(f\"Loading {HF_DATASET}\")\n",
    "    raw = load_dataset(HF_DATASET)\n",
    "    df_tr, df_te = pd.DataFrame(raw[\"train\"]), pd.DataFrame(raw[\"test\"])\n",
    "    print(f\"  Raw — train: {len(df_tr)}, test: {len(df_te)}\")\n",
    "\n",
    "    REQUIRED = {\"prompt\", \"result\", \"score_s1_rd1\", \"score_s6_custom\",\n",
    "                \"analysis_rd1\", \"analysis_custom\"}\n",
    "    assert not (REQUIRED - set(df_tr.columns)), f\"Missing columns: {REQUIRED - set(df_tr.columns)}\"\n",
    "\n",
    "    if sample_size:\n",
    "        df_tr = df_tr.sample(min(sample_size, len(df_tr)), random_state=SEED).reset_index(drop=True)\n",
    "        df_te = df_te.sample(min(sample_size//4, len(df_te)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    PREFIXES = [f\"score_s{i}_\" for i in range(1, 7)]\n",
    "    ROUNDS = [\"rd1\", \"rd2\", \"rd3\", \"custom\"]\n",
    "    PAIRS  = [(\"rd1\",\"rd2\"),(\"rd1\",\"rd3\"),(\"rd1\",\"custom\"),\n",
    "              (\"rd2\",\"rd3\"),(\"rd2\",\"custom\"),(\"rd3\",\"custom\")]\n",
    "\n",
    "    def _indent(s): return \"\\n\".join(\"    \" + l for l in s.splitlines())\n",
    "\n",
    "    def _build(df, split):\n",
    "        sft, dpo, skipped = [], [], 0\n",
    "        for _, row in df.iterrows():\n",
    "            code = \"<buggy_code>\\n\" + (row[\"prompt\"] + _indent(row[\"result\"])).strip(\"\\n\") + \"\\n</buggy_code>\"\n",
    "            instr = INSTRUCTION + \"\\n\" + code\n",
    "            info = {}\n",
    "            for rd in ROUNDS:\n",
    "                sc = sum(_parse_score(row.get(p+rd)) for p in PREFIXES) / 42\n",
    "                txt = row.get(\"analysis_\"+rd, \"\")\n",
    "                if txt is None or (isinstance(txt, float) and pd.isna(txt)): txt = \"\"\n",
    "                info[rd] = {\"analysis\": str(txt), \"score\": sc}\n",
    "            for r1, r2 in PAIRS:\n",
    "                if info[r1][\"score\"] == info[r2][\"score\"]: continue\n",
    "                b = r1 if info[r1][\"score\"] > info[r2][\"score\"] else r2\n",
    "                w = r2 if b == r1 else r1\n",
    "                if not info[b][\"analysis\"].strip() or not info[w][\"analysis\"].strip():\n",
    "                    skipped += 1; continue\n",
    "                sft.append({\"messages\": [\n",
    "                    {\"content\": instr, \"role\": \"user\"},\n",
    "                    {\"content\": info[b][\"analysis\"], \"role\": \"assistant\"}]})\n",
    "                dpo.append({\"prompt\": instr,\n",
    "                    \"chosen\":   [{\"content\": instr, \"role\": \"user\"},\n",
    "                                 {\"content\": info[b][\"analysis\"], \"role\": \"assistant\"}],\n",
    "                    \"rejected\": [{\"content\": instr, \"role\": \"user\"},\n",
    "                                 {\"content\": info[w][\"analysis\"], \"role\": \"assistant\"}],\n",
    "                    \"metadata\": [{\"chosen\": b, \"rejected\": w}]})\n",
    "        if skipped: print(f\"  ⚠ {split}: skipped {skipped} pairs (empty analysis)\")\n",
    "        return sft, dpo\n",
    "\n",
    "    s_tr, d_tr = _build(df_tr, \"train\")\n",
    "    s_te, d_te = _build(df_te, \"test\")\n",
    "\n",
    "    ds_sft = DatasetDict({\"train\": Dataset.from_pandas(pd.DataFrame(s_tr)),\n",
    "                           \"test\":  Dataset.from_pandas(pd.DataFrame(s_te))})\n",
    "    ds_dpo = DatasetDict({\"train\": Dataset.from_pandas(pd.DataFrame(d_tr)),\n",
    "                           \"test\":  Dataset.from_pandas(pd.DataFrame(d_te))})\n",
    "\n",
    "    print(f\"  SFT — train: {len(s_tr)}, test: {len(s_te)}\")\n",
    "    print(f\"  DPO — train: {len(d_tr)}, test: {len(d_te)}\")\n",
    "    return ds_sft, ds_dpo\n",
    "\n",
    "def format_sft(ds, tok):\n",
    "    cols = list(ds[\"train\"].features)\n",
    "    return ds.map(apply_chat_template, fn_kwargs={\"task\": \"sft\"},\n",
    "                  num_proc=NUM_PROC, remove_columns=cols)\n",
    "\n",
    "def format_dpo(ds, tok):\n",
    "    cols = list(ds[\"train\"].features)\n",
    "    fmt = ds.map(apply_chat_template, fn_kwargs={\"task\": \"dpo\"},\n",
    "                 num_proc=NUM_PROC, remove_columns=cols)\n",
    "    return fmt.rename_columns({\"text_prompt\": \"prompt\",\n",
    "                               \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"})\n",
    "\n",
    "def _score_completion(model, tokenizer, prompt_text, completion_text, device):\n",
    "    \"\"\"Length-normalised conditional log-prob of the completion.\"\"\"\n",
    "    full_enc = tokenizer(prompt_text + completion_text, return_tensors=\"pt\",\n",
    "                         truncation=True, max_length=MAX_SEQ_LENGTH).to(device)\n",
    "    prompt_len = tokenizer(prompt_text, return_tensors=\"pt\",\n",
    "                           truncation=True, max_length=MAX_SEQ_LENGTH).input_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**full_enc).logits\n",
    "\n",
    "    if prompt_len >= full_enc.input_ids.shape[1]:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    comp_logits = logits[:, prompt_len - 1:-1, :]\n",
    "    comp_labels = full_enc.input_ids[:, prompt_len:]\n",
    "    log_probs = torch.nn.functional.log_softmax(comp_logits, dim=-1)\n",
    "    return log_probs.gather(-1, comp_labels.unsqueeze(-1)).squeeze(-1).mean().item()\n",
    "\n",
    "def run_eval(model, tokenizer, dpo_raw_dataset, max_items=None, no_iter=1):\n",
    "    \"\"\"Pairwise preference accuracy and mean margin (multi-pass).\"\"\"\n",
    "    was_training = model.training; model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    N = len(dpo_raw_dataset) if max_items is None else min(max_items, len(dpo_raw_dataset))\n",
    "    total_correct, pass_accs, margins = 0, [], []\n",
    "\n",
    "    for it in range(no_iter):\n",
    "        correct = 0\n",
    "        for i in tqdm(range(N), desc=f\"Eval {it+1}/{no_iter}\", leave=False):\n",
    "            item = dpo_raw_dataset[i]\n",
    "            fmt = apply_chat_template(\n",
    "                {\"chosen\": item[\"chosen\"], \"rejected\": item[\"rejected\"]}, task=\"dpo\")\n",
    "            r_ch = _score_completion(model, tokenizer, fmt[\"text_prompt\"], fmt[\"text_chosen\"], device)\n",
    "            r_re = _score_completion(model, tokenizer, fmt[\"text_prompt\"], fmt[\"text_rejected\"], device)\n",
    "            margins.append(r_ch - r_re)\n",
    "            if r_ch > r_re:\n",
    "                correct += 1\n",
    "\n",
    "        pass_accs.append(correct / max(1, N))\n",
    "        total_correct += correct\n",
    "\n",
    "    if was_training: model.train()\n",
    "    mean_acc = total_correct / max(1, no_iter * N)\n",
    "    mean_margin = float(np.mean(margins))\n",
    "    return mean_acc, pass_accs, mean_margin\n",
    "\n",
    "_PAL = {\"Baseline\": \"#95a5a6\", \"SFT\": \"#3498db\", \"DPO\": \"#27ae60\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3569ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "for _attempt in range(3):\n",
    "    try:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=MODEL_NAME, max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dtype=None, load_in_4bit=LOAD_IN_4BIT)\n",
    "        print(f\"Loaded {MODEL_NAME}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if _attempt < 2:\n",
    "            print(f\"Attempt {_attempt+1} failed ({e.__class__.__name__}), retrying...\")\n",
    "            import time; time.sleep(10)\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76800ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sft, ds_dpo = make_datasets(sample_size=SAMPLE_SIZE)\n",
    "sft_fmt = format_sft(ds_sft, tokenizer)\n",
    "dpo_fmt = format_dpo(ds_dpo, tokenizer)\n",
    "\n",
    "assert len(sft_fmt[\"train\"]) > 0 and len(dpo_fmt[\"train\"]) > 0, \"Empty dataset\"\n",
    "assert \"text\" in sft_fmt[\"train\"].features and \"prompt\" in dpo_fmt[\"train\"].features\n",
    "\n",
    "print(f\"\\nSFT  {len(sft_fmt['train']):>5} train / {len(sft_fmt['test']):>4} test\")\n",
    "print(f\"DPO  {len(dpo_fmt['train']):>5} train / {len(dpo_fmt['test']):>4} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca97b92",
   "metadata": {},
   "source": [
    "## Stage 1 — SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SFT:\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(model, r=64,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                        \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        lora_alpha=64, lora_dropout=0, bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\", random_state=SEED)\n",
    "\n",
    "    sft_trainer = SFTTrainer(model=model, tokenizer=tokenizer,\n",
    "        train_dataset=sft_fmt[\"train\"],\n",
    "        args=SFTConfig(\n",
    "            dataset_text_field=\"text\", max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dataset_num_proc=NUM_PROC, packing=False,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRAD_ACCUM,\n",
    "            dataloader_num_workers=DL_WORKERS,\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_prefetch_factor=2,\n",
    "            warmup_steps=5, num_train_epochs=SFT_EPOCHS,\n",
    "            learning_rate=2e-4,\n",
    "            tf32=USE_TF32, fp16=USE_FP16, bf16=USE_BF16,\n",
    "            logging_steps=10, optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01, lr_scheduler_type=\"linear\",\n",
    "            seed=SEED, output_dir=\"outputs/sft_demo\"),\n",
    "    )\n",
    "\n",
    "    clear_gpu()\n",
    "    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n",
    "    _t0 = time.time()\n",
    "    sft_trainer.train()\n",
    "    _sft_dur = time.time() - _t0\n",
    "\n",
    "    model.save_pretrained(\"outputs/sft_demo\")\n",
    "    tokenizer.save_pretrained(\"outputs/sft_demo\")\n",
    "    # Merge LoRA into base so DPO can use SFT as the implicit reference\n",
    "    model.save_pretrained_merged(\"outputs/sft_merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "    print(f\"SFT done → outputs/sft_demo + sft_merged  ({_sft_dur/60:.1f} min, {gpu_mem()})\")\n",
    "else:\n",
    "    print(\"SFT skipped (RUN_SFT=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43459de",
   "metadata": {},
   "source": [
    "## Stage 2 — DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09910d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT baseline accuracy (before DPO)\n",
    "# (skip for_inference() here — it would break subsequent DPO training)\n",
    "if RUN_SFT:\n",
    "    sft_baseline, _, sft_bl_margin = run_eval(model, tokenizer, ds_dpo[\"test\"],\n",
    "                                              max_items=min(128, len(ds_dpo[\"test\"])), no_iter=1)\n",
    "    print(f\"SFT baseline accuracy: {sft_baseline * 100:.2f}%  margin: {sft_bl_margin:.4f}\")\n",
    "else:\n",
    "    sft_baseline, sft_bl_margin = None, None\n",
    "    print(\"SFT was skipped — no baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DPO:\n",
    "    try: del sft_trainer\n",
    "    except NameError: pass\n",
    "    clear_gpu()\n",
    "\n",
    "    from unsloth import PatchDPOTrainer; PatchDPOTrainer()\n",
    "    from trl import DPOTrainer, DPOConfig\n",
    "    from transformers import TrainerCallback\n",
    "\n",
    "    # ── Reload merged SFT as base → fresh LoRA for DPO ────────────\n",
    "    # ref_model=None uses base (LoRA disabled) as reference.\n",
    "    # If we keep SFT’s LoRA, log(π_SFT/π_base) is already large\n",
    "    # → sigmoid saturates → DPO gradients vanish.\n",
    "    # Merging SFT into base then adding fresh LoRA makes ref ≈ SFT.\n",
    "    if RUN_SFT:\n",
    "        del model; clear_gpu()\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            \"outputs/sft_merged\", max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dtype=torch.bfloat16 if USE_BF16 else None,\n",
    "            load_in_4bit=LOAD_IN_4BIT)\n",
    "        print(\"Loaded merged SFT as DPO base\")\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(model, r=64,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                        \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        lora_alpha=64, lora_dropout=0, bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\", random_state=SEED)\n",
    "\n",
    "    # ── Per-epoch preference eval + early stopping callback ───────────\n",
    "    epoch_log = []\n",
    "\n",
    "    class _PrefEvalCB(TrainerCallback):\n",
    "        \"\"\"Evaluate pairwise preference accuracy after every epoch and\n",
    "        trigger early stopping when accuracy plateaus.\"\"\"\n",
    "        def __init__(self, patience=3):\n",
    "            self.patience = patience\n",
    "            self.best_acc, self.best_epoch, self.wait = 0.0, 0, 0\n",
    "            self._t0 = None\n",
    "\n",
    "        def on_epoch_begin(self, args, state, control, **kw):\n",
    "            self._t0 = time.time()\n",
    "\n",
    "        def on_epoch_end(self, args, state, control, model=None, **kw):\n",
    "            ep = int(state.epoch)\n",
    "            acc, pass_accs, margin = run_eval(\n",
    "                model, tokenizer, ds_dpo[\"test\"], no_iter=EVAL_PASSES)\n",
    "            _dur = (time.time() - self._t0) / 60\n",
    "\n",
    "            ckpt = f\"outputs/dpo_demo/epoch_{ep}\"\n",
    "            os.makedirs(ckpt, exist_ok=True)\n",
    "            dpo_trainer.save_model(ckpt)\n",
    "\n",
    "            tag = \"\"\n",
    "            if acc > self.best_acc:\n",
    "                self.best_acc, self.best_epoch = acc, ep\n",
    "                self.wait = 0\n",
    "                dpo_trainer.save_model(\"outputs/dpo_demo\")\n",
    "                tag = \" ★\"\n",
    "            else:\n",
    "                self.wait += 1\n",
    "\n",
    "            epoch_log.append({\"epoch\": ep, \"acc\": acc, \"margin\": margin,\n",
    "                              \"pass_accs\": pass_accs, \"time_min\": _dur})\n",
    "            print(f\"  Epoch {ep:>2}/{DPO_EPOCHS}  acc={acc*100:.2f}%  \"\n",
    "                  f\"margin={margin:.4f}  {_dur:.1f}min{tag}\")\n",
    "\n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"  Early stop — no improvement for {self.patience} epochs\")\n",
    "                control.should_training_stop = True\n",
    "\n",
    "    _cb = _PrefEvalCB(patience=PATIENCE)\n",
    "\n",
    "    dpo_trainer = DPOTrainer(model=model, ref_model=None,\n",
    "        args=DPOConfig(\n",
    "            per_device_train_batch_size=DPO_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=DPO_GRAD_ACCUM,\n",
    "            dataloader_num_workers=DL_WORKERS,\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_prefetch_factor=2,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=DPO_EPOCHS,\n",
    "            learning_rate=DPO_LR,\n",
    "            tf32=USE_TF32, fp16=USE_FP16, bf16=USE_BF16,\n",
    "            logging_steps=1, optim=\"adamw_8bit\",\n",
    "            weight_decay=0.0,\n",
    "            max_grad_norm=1.0,\n",
    "            beta=0.05, max_length=DPO_MAX_LEN, max_prompt_length=DPO_PROMPT_LEN,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            save_strategy=\"no\",\n",
    "            seed=42, output_dir=\"outputs/dpo_demo\"),\n",
    "        train_dataset=dpo_fmt[\"train\"], tokenizer=tokenizer,\n",
    "        callbacks=[_cb],\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n",
    "    _dpo_t0 = time.time()\n",
    "    dpo_trainer.train()\n",
    "    _dpo_total = time.time() - _dpo_t0\n",
    "\n",
    "    best_acc, best_epoch = _cb.best_acc, _cb.best_epoch\n",
    "    print(f\"\\nBest epoch: {best_epoch} — {best_acc*100:.2f}%\")\n",
    "    print(f\"DPO total: {_dpo_total/60:.1f} min | {gpu_mem()}\")\n",
    "\n",
    "    # push to Hub\n",
    "    HUB_REPO = \"t4gandhi/Codellama-7b-fine-tuned\"\n",
    "    model.push_to_hub(HUB_REPO)\n",
    "    tokenizer.push_to_hub(HUB_REPO)\n",
    "    print(f\"Pushed → https://huggingface.co/{HUB_REPO}\")\n",
    "else:\n",
    "    epoch_log = []\n",
    "    print(\"DPO skipped (RUN_DPO=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e39ced",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_max = len(ds_dpo[\"test\"])\n",
    "results, all_margins = {}, {}\n",
    "HUB_DPO = \"t4gandhi/Codellama-7b-fine-tuned\"\n",
    "\n",
    "print(f\"Evaluating {eval_max} test pairs × {EVAL_PASSES} passes\\n\")\n",
    "\n",
    "# 1. Baseline (reload pretrained weights)\n",
    "if RUN_SFT or RUN_DPO:\n",
    "    try: del model\n",
    "    except NameError: pass\n",
    "    clear_gpu()\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME, max_seq_length=MAX_SEQ_LENGTH, dtype=None,\n",
    "        load_in_4bit=LOAD_IN_4BIT)\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "acc_v, _, margin_v = run_eval(model, tokenizer, ds_dpo[\"test\"],\n",
    "                               max_items=eval_max, no_iter=EVAL_PASSES)\n",
    "results[\"Baseline\"], all_margins[\"Baseline\"] = acc_v, margin_v\n",
    "print(f\"  Baseline: {acc_v*100:.2f}%  margin={margin_v:.4f}\")\n",
    "del model; clear_gpu()\n",
    "\n",
    "# 2. SFT checkpoint (LoRA adapter → bf16 cast fixes dtype mismatch)\n",
    "sft_path = Path(\"outputs/sft_demo\")\n",
    "if sft_path.exists():\n",
    "    m, t = FastLanguageModel.from_pretrained(str(sft_path), max_seq_length=MAX_SEQ_LENGTH,\n",
    "                                             dtype=None, load_in_4bit=LOAD_IN_4BIT)\n",
    "    if USE_BF16: m = m.to(torch.bfloat16)          # cast all layers to bf16\n",
    "    FastLanguageModel.for_inference(m)\n",
    "    acc_s, _, margin_s = run_eval(m, t, ds_dpo[\"test\"], max_items=eval_max,\n",
    "                                   no_iter=EVAL_PASSES)\n",
    "    results[\"SFT\"], all_margins[\"SFT\"] = acc_s, margin_s\n",
    "    print(f\"  SFT     : {acc_s*100:.2f}%  margin={margin_s:.4f}\")\n",
    "    del m, t; clear_gpu()\n",
    "else:\n",
    "    results[\"SFT\"] = None; print(\"  SFT checkpoint not found\")\n",
    "\n",
    "# 3. DPO (local or Hub fallback)\n",
    "dpo_path = Path(\"outputs/dpo_demo\")\n",
    "dpo_src = str(dpo_path) if dpo_path.exists() else HUB_DPO\n",
    "if dpo_src:\n",
    "    m, t = FastLanguageModel.from_pretrained(dpo_src, max_seq_length=MAX_SEQ_LENGTH,\n",
    "                                             dtype=torch.bfloat16 if USE_BF16 else None,\n",
    "                                             load_in_4bit=LOAD_IN_4BIT)\n",
    "    if USE_BF16: m = m.to(torch.bfloat16)          # safety cast\n",
    "                                             dtype=None, load_in_4bit=LOAD_IN_4BIT)\n",
    "    if USE_BF16: m = m.to(torch.bfloat16)          # cast all layers to bf16\n",
    "    FastLanguageModel.for_inference(m)\n",
    "    acc_d, _, margin_d = run_eval(m, t, ds_dpo[\"test\"], max_items=eval_max,\n",
    "                                   no_iter=EVAL_PASSES)\n",
    "    results[\"DPO\"], all_margins[\"DPO\"] = acc_d, margin_d\n",
    "    print(f\"  DPO     : {acc_d*100:.2f}%  margin={margin_d:.4f}  ({dpo_src})\")\n",
    "    del m, t; clear_gpu()\n",
    "else:\n",
    "    results[\"DPO\"] = None; print(\"  DPO checkpoint not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"  {'Stage':<8} {'Accuracy':>10} {'Margin':>10}\")\n",
    "print(f\"  {'-'*8} {'-'*10} {'-'*10}\")\n",
    "for k, v in results.items():\n",
    "    acc_str = f\"{v*100:.2f}%\" if v is not None else \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90aa3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1 — Stage Comparison: Accuracy + Preference Margin\n",
    "stages = [k for k in [\"Baseline\", \"SFT\", \"DPO\"] if results.get(k) is not None]\n",
    "accs   = [results[s] * 100 for s in stages]\n",
    "mars   = [all_margins[s] for s in stages]\n",
    "colors = [_PAL[s] for s in stages]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4), gridspec_kw={\"wspace\": 0.35})\n",
    "\n",
    "# Left — accuracy\n",
    "bars1 = ax1.bar(stages, accs, color=colors, edgecolor=\"black\", lw=0.7, width=0.5)\n",
    "for bar, a in zip(bars1, accs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.2,\n",
    "             f\"{a:.1f}%\", ha=\"center\", va=\"bottom\", fontweight=\"bold\", fontsize=11)\n",
    "ax1.axhline(50, ls=\"--\", color=\"#e74c3c\", lw=0.9, alpha=0.6, label=\"Random baseline\")\n",
    "ax1.set_ylabel(\"Pairwise Preference Accuracy (%)\")\n",
    "ax1.set_title(\"(a)  Accuracy by Training Stage\", fontweight=\"bold\", pad=10)\n",
    "ax1.set_ylim(0, max(accs) + 18)\n",
    "ax1.legend(loc=\"upper left\", fontsize=8)\n",
    "\n",
    "# Right — margin\n",
    "bars2 = ax2.bar(stages, mars, color=colors, edgecolor=\"black\", lw=0.7, width=0.5)\n",
    "for bar, m in zip(bars2, mars):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "             f\"{m:.4f}\", ha=\"center\", va=\"bottom\", fontweight=\"bold\", fontsize=10)\n",
    "ax2.axhline(0, ls=\"--\", color=\"#333\", lw=0.7, alpha=0.4)\n",
    "ax2.set_ylabel(\"Mean Preference Margin\")\n",
    "ax2.set_title(\"(b)  Preference Margin by Training Stage\", fontweight=\"bold\", pad=10)\n",
    "y_pad = max(abs(min(mars)), max(mars)) * 0.35\n",
    "ax2.set_ylim(min(min(mars) - y_pad, -0.01), max(mars) + y_pad)\n",
    "\n",
    "fig.suptitle(\"CodeLlama-7B — Effect of SFT and DPO on Preference Alignment\",\n",
    "             fontweight=\"bold\", fontsize=13, y=1.03)\n",
    "plt.tight_layout()\n",
    "for ext in (\"pdf\", \"png\"): fig.savefig(FIGURE_DIR / f\"fig1_stage_comparison.{ext}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2 — DPO Training Dynamics\n",
    "if epoch_log:\n",
    "    df_log = pd.DataFrame(epoch_log)\n",
    "    epochs     = df_log[\"epoch\"].values\n",
    "    means      = df_log[\"acc\"].values * 100\n",
    "    ep_margins = df_log[\"margin\"].values\n",
    "\n",
    "    fig, (ax, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # (a) Accuracy over epochs\n",
    "    ax.plot(epochs, means, \"o-\", color=\"#27ae60\", lw=2, ms=5)\n",
    "    ax.axhline(50, ls=\"--\", color=\"#e74c3c\", lw=0.8, alpha=0.5, label=\"Random\")\n",
    "    if sft_baseline is not None:\n",
    "        ax.axhline(sft_baseline * 100, ls=\":\", color=\"#3498db\", lw=1.2,\n",
    "                   label=f\"SFT ({sft_baseline*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"DPO Epoch\"); ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(\"(a)  Pairwise Accuracy\", fontweight=\"bold\")\n",
    "    ax.legend(fontsize=8); ax.set_ylim(0, 100); ax.set_xticks(epochs)\n",
    "\n",
    "    # (b) Margin over epochs\n",
    "    ax2.plot(epochs, ep_margins, \"s-\", color=\"#8e44ad\", lw=2, ms=5)\n",
    "    ax2.axhline(0, ls=\"--\", color=\"#333\", lw=0.6, alpha=0.4)\n",
    "    if sft_bl_margin is not None:\n",
    "        ax2.axhline(sft_bl_margin, ls=\":\", color=\"#3498db\", lw=1.2,\n",
    "                    label=f\"SFT ({sft_bl_margin:.4f})\")\n",
    "    ax2.set_xlabel(\"DPO Epoch\"); ax2.set_ylabel(\"Mean Preference Margin\")\n",
    "    ax2.set_title(\"(b)  Preference Margin\", fontweight=\"bold\")\n",
    "    ax2.legend(fontsize=8); ax2.set_xticks(epochs)\n",
    "\n",
    "    fig.suptitle(\"CodeLlama-7B — DPO Training Dynamics\",\n",
    "                 fontweight=\"bold\", fontsize=12, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    for ext in (\"pdf\", \"png\"): fig.savefig(FIGURE_DIR / f\"fig2_dpo_training.{ext}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No epoch log — Figure 2 skipped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f69e1e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Stage | Method | Description |\n",
    "|-------|--------|-------------|\n",
    "| Baseline | Baseline | Pretrained CodeLlama-7B, no preference signal |\n",
    "| Stage 1 | SFT | Supervised fine-tuning on highest-scored analysis |\n",
    "| Stage 2 | DPO | Direct preference optimisation on pairwise preferences |\n",
    "\n",
    "| Figure | File | Content |\n",
    "|--------|------|---------|\n",
    "| Fig 1 | `fig1_stage_comparison.pdf` | Accuracy + margin comparison across all stages |\n",
    "| Fig 2 | `fig2_dpo_training.pdf` | Per-epoch accuracy and margin during DPO training |\n",
    "\n",
    "**Reproducibility:** H100 80 GB, TF32 enabled, `SAMPLE_SIZE = None`, `EVAL_PASSES = 5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = {\n",
    "    \"model_name\": \"CodeLlama-7B\",\n",
    "    \"model_id\": MODEL_NAME,\n",
    "    \"results\": dict(results),\n",
    "    \"margins\": dict(all_margins),\n",
    "    \"epoch_log\": [{\"epoch\": e[\"epoch\"], \"acc\": e[\"acc\"], \"margin\": e[\"margin\"],\n",
    "                   \"pass_accs\": e[\"pass_accs\"], \"time_min\": e[\"time_min\"]}\n",
    "                  for e in epoch_log] if epoch_log else [],\n",
    "    \"config\": {\"sft_epochs\": SFT_EPOCHS, \"dpo_epochs\": DPO_EPOCHS,\n",
    "               \"batch_size\": BATCH_SIZE, \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "               \"sample_size\": SAMPLE_SIZE, \"eval_passes\": EVAL_PASSES},\n",
    "}\n",
    "\n",
    "out_path = Path(\"outputs/results_codellama_7b.json\")\n",
    "out_path.parent.mkdir(exist_ok=True)\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(export, f, indent=2)\n",
    "\n",
    "print(f\"Exported → {out_path}\")\n",
    "for k, v in results.items():\n",
    "    acc_str = f\"{v*100:.2f}%\" if v is not None else \"N/A\"\n",
    "    mar_str = f\"{all_margins[k]:.4f}\" if k in all_margins else \"N/A\"\n",
    "    print(f\"  {k}: acc={acc_str}  margin={mar_str}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
